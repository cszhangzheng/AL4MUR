
<html>
    <!-- head -->
    <head>
        <title>ACM ICMR 2022 | AL4MUR</title>
    </head>
    
    <!-- body -->
    <body style="text-align: justify; background-color: #fcfcfc; font-family: Raleway;">
        <div class="header">
            <h1><a href="https://www.icmr2022.org/" class="header-link">ACM ICMR 2022</a> Special Session</h1>
            <h2>Adversarial Learning for Multimedia Understanding and Retrieval</h2>
        </div>

        <div class="page">
            <p>Nowadays we are living in the era of big multimedia data. The adversarial characteristic naturally lives in many multimedia understanding and retrieval models. Adversarial machine learning has become a cutting-edge technique that attempts to fool the learning systems by generating deceptive data with imperceptible perturbations. Particularly, the existence of adversarial examples destroys the reliability and robustness of deep neural networks, which almost impedes the practical deployment of deep learning models in various multimedia systems. It has been well demonstrated that the existing deep learning models are vulnerable to carefully crafted attacks from malicious adversaries. Moreover, such a security challenge goes well beyond the simple multimedia systems that could all potentially be subject to adversarial attacks.</p>
            <p>This ICMR special session is devoted to the publication of high-quality research papers on adversarial learning for various multimedia understanding and retrieval models. 
                The special session will seek original contributions, which address the key challenges and problems. The topics of interest include, but are not limited to:</p>
            <p>
                &emsp;- Adversarial learning for large-scale image/video/music/audio retrieval<br>
                &emsp;- Adversarial learning for cross-modal analysis and retrieval<br>
                &emsp;- Adversarial learning for image/video classification/detection/tracking<br>
                &emsp;- Adversarial learning techniques on multi-modal learning<br>
                &emsp;- Advanced techniques for generating adversarial examples<br>
                &emsp;- Adversarial attacks and defenses in multi-modal medical analysis<br>
                &emsp;- Adversarial attacks and defenses for autonomous vehicles<br>
                &emsp;- Privacy protection architectures in multi-modal data analysis<br>
                &emsp;- Adversarial examples for multi-modal biomedical tasks<br>
                &emsp;- Optimization algorithms in adversarial attacks and defenses<br>
                &emsp;- Security issues in deep learning-based multimedia database systems<br>
                &emsp;- New benchmark datasets for multimedia adversarial learning<br>
            </p>
            
            <h3>Maximum Length of a Paper</h3>
            <p>Each full paper should be limited to <b>6-8 pages (6 pages limit + references)</b>.</p>
            
            <h3>Important Dates</h3>
            <p>
                Paper Submission Deadline: <b><s>January 20, 2022</s> January 30, 2022</b>  (Extended!)<br>
                Notification of Acceptance: <b>March 30, 2022</b><br>
                Deadline for Camera Ready: <b>TBD</b><br>
                Main Conference: <b>June 27-30, 2022</b>
            </p>

            <h3>Submission Guideline</h3>
            <p>Submitted papers should present original, unpublished work, relevant to one of the topics of the Special Session. 
                Following the reviewing procedure of ICMR, all submitted papers will be evaluated on the basis of relevance, significance of contribution, technical quality, scholarship, and quality of presentation, 
                by at least three independent reviewers. </p>
            <p>Submissions should conform to the submission instructions of the <a href="https://www.icmr2022.org/authors/submissions/" class="page-link">ICMR 2022 Paper submission section</a>.</p>

            <h3>Organizers</h3>
            <p>
                &emsp;- Zheng Zhang, A/Prof., Harbin Institute of Technology, Shenzhen, China, email: <a href="mailto:darrenzz219@gmail.com" class="page-link">darrenzz219@gmail.com</a><br>
                &emsp;- Lei Zhu, Prof., Shandong Normal University, China, email: <a href="mailto:leizhu0608@gmail.com" class="page-link">leizhu0608@gmail.com</a><br>
                &emsp;- Shuihua Wang, Dr., University of Leicester, UK, email: <a href="mailto:shuihuawang@ieee.org" class="page-link">shuihuawang@ieee.org</a><br>
                &emsp;- M. Emre Celebi, Prof., University of Central Arkansas, USA, email: <a href="mailto:ecelebi@uca.edu" class="page-link">ecelebi@uca.edu</a><br>
            </p>
        </div>
    </body>

    <!-- style -->
    <style>
        @import url('https://fonts.googleapis.com/css?family=Open+Sans:300,400,600,700|Raleway:300,400,500,600,700');

        .header {
            color: white;
            background-color: #32669a;
            padding-left: 100px;
            padding-right: 100px;
            padding-top: 20px;
            padding-bottom: 20px;
        }
        .page {
            padding-left: 100px;
            padding-right: 100px;
            padding-top: 10px;
            padding-bottom: 100px;
        }

        a {
            text-decoration: none;
        }
        a.page-link:link {
            color: #7ea3c8;
        }
        a.page-link:visited {
            color: #7ea3c8;
        }
        a.page-link:hover {
            color: white;
            background-color: #3c6b9a;
        }
        a.header-link:link {
            color: white;
        }
        a.header-link:visited {
            color: white;
        }
        a.header-link:hover {
            color: #7ea3c8;
            background-color: white;
        }

        h3 {
            color: #32669a;
        }

        p{
            font-size: 18px;
            line-height: 26px;
        }
    </style>
</html>
